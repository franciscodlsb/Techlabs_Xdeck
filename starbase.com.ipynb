{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T06:09:47.943602Z",
     "start_time": "2021-01-24T06:09:47.393074Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T06:09:47.973523Z",
     "start_time": "2021-01-24T06:09:47.958563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract urls from website\n",
    "def get_company_links():\n",
    "        \n",
    "    template = 'https://www.startbase.com/startups/?listOptions%5Bcompany-startup%5D=%7B%22version%22%3A1.3%2C%22sort%22%3A%22company.startbase_score%22%2C%22sortDirection%22%3A%22desc%22%2C%22display%22%3A%22small%22%2C%22itemsPerPage%22%3A4567%2C%22page%22%3A{}%2C%22userLocation%22%3Anull%2C%22filters%22%3A%7B%7D%7D'\n",
    "    driver = webdriver.Firefox() # open browser\n",
    "\n",
    "    links = []\n",
    "    count = 1\n",
    "    \n",
    "    while True: # go through all urls\n",
    "        try:\n",
    "            url = template.format(count)\n",
    "            driver.get(url) # go to url\n",
    "            time.sleep(10) # wait for page to load\n",
    "            element = driver.find_element_by_class_name('main-body') # get where links are            \n",
    "            html = element.get_attribute('innerHTML') # obtain updated html with links\n",
    "            \n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            all_as = soup.find_all(\"a\")            \n",
    "            page_links = []\n",
    "            for a in all_as:\n",
    "                link = a.get(\"href\")\n",
    "                if link is not None and link.startswith('/organization/'):\n",
    "                    page_links.append(link)           \n",
    "            \n",
    "            if not page_links: # when there are no more startups in the url\n",
    "                driver.quit() # close browser\n",
    "                break\n",
    "            \n",
    "            links = links + page_links\n",
    "            count += 1    \n",
    "            \n",
    "        except:\n",
    "            driver.quit()\n",
    "            break\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T06:09:50.050296Z",
     "start_time": "2021-01-24T06:09:49.968484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the info from a company url\n",
    "def get_company_info(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Name, logo and teaser\n",
    "    title_card = soup.find('div', 'flex-grow-1 mx-lg-4')\n",
    "    startup_name = title_card.h1.get('data-name')\n",
    "    logo = title_card.h1.get('data-icon')\n",
    "    teaser = title_card.find('div', 'sb-teaser').text.strip()\n",
    "    \n",
    "    # Body info\n",
    "    body_card = soup.find('ul', 'unformated row justify-content-start mb-0')\n",
    "    info_cards = body_card.find_all('li')\n",
    "    startup_info = []; labels_info = []\n",
    "\n",
    "    for count, info in enumerate(info_cards):\n",
    "        labels_info.append(info.text.strip().split(':')[0])\n",
    "        try:\n",
    "            if count != 9: #10th item is links to social media\n",
    "                info_piece = info.text.strip()\n",
    "                # text cleaning\n",
    "            #         if type(info_piece) == str: # When empty\n",
    "            #             info_piece = ''\n",
    "                if '\\n ' in info_piece:\n",
    "                    info_piece = info_piece.split('\\n ')[1]\n",
    "                else:\n",
    "                    info_piece = info_piece.split(': ')[1]\n",
    "\n",
    "                if info_piece[0] == ' ':\n",
    "                    info_piece = info_piece[1::]   \n",
    "            else: #social media\n",
    "                media_links = []\n",
    "                for media in info.find_all('a'):\n",
    "                    media_links.append(media.get('href'))\n",
    "                info_piece = media_links\n",
    "        except:\n",
    "            info_piece = ''\n",
    "        \n",
    "        startup_info.append(info_piece)\n",
    "\n",
    "    # About info\n",
    "    try:\n",
    "        about_company = soup.find('div', {'class':'sb-card mb-4'}, id='company').p.text\n",
    "    except:\n",
    "        about_company = ''\n",
    "\n",
    "    # Team info\n",
    "    try:\n",
    "        # Get names\n",
    "        member_names = soup.find_all('span',{'class':'name'})\n",
    "        names = [];\n",
    "        for name in member_names:\n",
    "            names.append(name.text)\n",
    "        # Get labels\n",
    "        member_labels = soup.find_all('span',{'class':'label'})\n",
    "        labels = [];\n",
    "        for label in member_labels:\n",
    "            labels.append(label.text) \n",
    "\n",
    "        team_members = dict(zip(names, labels))\n",
    "    except:\n",
    "        team_members = '';\n",
    "    \n",
    "    # Join extracted data\n",
    "    info_data = [startup_name] + [logo] + [teaser] + startup_info + [about_company] + [team_members]\n",
    "    info_labels = ['Name'] + ['Logo'] + ['Teaser'] + labels_info + ['About'] + ['Members']\n",
    "    \n",
    "    company_data = dict(zip(info_labels, info_data))\n",
    "    \n",
    "    return company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T06:28:39.373532Z",
     "start_time": "2021-01-24T06:09:57.057286Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain all links of startups\n",
    "links = get_company_links()\n",
    "with open('links.pickle', 'wb') as f:\n",
    "    pickle.dump(links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T13:59:14.813110Z",
     "start_time": "2021-01-24T13:46:14.525970Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all data from links\n",
    "data = []\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    data.append(get_company_info('https://www.startbase.com' + link))\n",
    "    if i % 50 == 0:\n",
    "        print(i)     \n",
    "dataframe = pd.DataFrame(data)\n",
    "with open('startbasedata.pickle', 'wb') as f:\n",
    "    pickle.dump(dataframe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T14:02:23.080181Z",
     "start_time": "2021-01-24T14:02:22.944544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to csv\n",
    "dataframe.to_csv('startbase.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
